---
title: "Demonstration of Caret for CaRT"
author: "JAS/jck2183-note"
date: "2021/2/16 "
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Overview of the Caret Package

The caret package (Classification And REgression Training) contains a number of functions to streamline the process for creating analytic pipelines for prediction. It calls to other libraries to run algorithms, but provides a seamless and uniform interface for working with different algorithms.

Primary functionalities of caret include:

* pre-processing
* data splitting
* feature selection
* model tuning using resampling
* variable importance estimation

***

Helpful resources using caret:

Max Kuhn's explainer of the caret package
https://topepo.github.io/caret/model-training-and-tuning.html

Kuhn M. Building predictive models in R using the caret package. Journal of Statistical Software 2008;28(5) doi: 10.18637/jss.v028.i05

Webinar, given by Max Kuhn, available on YouTube (~1 hour): https://www.youtube.com/watch?v=7Jbb2ItbTC4


### Some useful functions for pre-processing
```{r preprocess}
library(tidyverse)
library(caret)
library(stats)

#Read in data on liver function study
set.seed(111)
hcvdat0 <- read.csv("./data/hcvdat0 .csv")
#Make outcome category a factor var
hcvdat0$Category<-as.factor(hcvdat0$Category)

#Collapse factor levels of outcome variable
hcvdat0$outcome.class<-fct_collapse(hcvdat0$Category, NED=c("0=Blood Donor","0s=suspect Blood Donor"), LiverDisease=c("1=Hepatitis", "2=Fibrosis", "3=Cirrhosis"))

#Drop category 
hcvdat0$Category<-NULL
hcvdat0$X<-NULL
hcvdat0<-na.omit(hcvdat0)

#Finding correlated predictors
hcvdat.numeric<- hcvdat0 %>% dplyr::select(where(is.numeric))
correlations<-cor(hcvdat.numeric, use="complete.obs")
high.correlations<-findCorrelation(correlations, cutoff=0.4)

#Remove highly correlated features
new.data.low.corr<-hcvdat.numeric[,-high.correlations]


#Centering and Scaling
set.up.preprocess<-preProcess(hcvdat.numeric, method=c("center", "scale"))
#Output pre-processed values
transformed.vals<-predict(set.up.preprocess, hcvdat.numeric)

#Creating balanced partitions in the data
train.index<-createDataPartition(hcvdat0$outcome.class, p=0.7, list=FALSE)

hcvdat.train<-hcvdat0[train.index,]
hcvdat.test<-hcvdat0[-train.index,]


#Construct k-folds in your data
train.folds<-createFolds(hcvdat0$outcome.class, k=10, list=FALSE)

```

### Model Training and Tuning


```{r models}

names(getModelInfo())

modelLookup("rpart")
modelLookup("adaboost")

## Cant use for regression but can use for classification.
#Train Function: used for tuning of hyperparameters and choosing "optimal" model

#Use trainControl Function to set method

#Perform `10-fold` cross-validation
control.settings<-trainControl(method="cv", number=10)

#Perform repeated 10-fold cross-validation
control.settings.b<-trainControl(method="repeatedcv", number=10, repeats=10)

#Perform sampling to balance data
control.settings.c<-trainControl(method="repeatedcv", number=10, repeats=10, sampling="down")

#Add into train function
set.seed(123)
lasso <- train(
 outcome.class ~., data = hcvdat.train, method = "glmnet",
  trControl = control.settings.c)

#Add tuning grid for lambda
lambda<-10^seq(-3,1, length=100)
lambda.grid<-expand.grid(alpha=1, lambda=lambda)

#Incorporate tuneGrid into train function
set.seed(123)
lasso.2 <- train(
 outcome.class ~., data = hcvdat.train, method = "glmnet",
  trControl = control.settings.c, tuneGrid = lambda.grid)


#Use plot to visualize tuning
plot(lasso.2)

#summaryFunction will allow calculation of sensitivity and specificity, classProbs= TRUE will allow the calculation of predicted probabilities

control.settings.d<-trainControl(method="repeatedcv", number=10, repeats=5, sampling="down", classProbs = TRUE, summaryFunction = twoClassSummary)

#Incorporate tuneGrid into train function
set.seed(123)
lasso.2 <- train(
 outcome.class ~., data = hcvdat.train, method = "glmnet",
  trControl = control.settings.d, tuneGrid = lambda.grid, metric="ROC")
  
lasso.2$bestTune

#The tolerance function could be used to find a less complex model based on (x-xbest)/xbestx 100, which is #the percent difference. For example, to select parameter values based on a 2% loss of performance:

whichTwoPct <- tolerance(lasso.2$results, metric = "ROC", 
                         tol = 2, maximize = TRUE) ## smaller shrinkage based on lambda difference

lasso.2$results[whichTwoPct,1:6]


```
#### The tolerance function could be used to find a less complex model based on (x-xbest)/xbestx 100, which is #the percent difference.

### Model Evaluation

```{r}

test.outcome<-predict(lasso.2, hcvdat.test)

confusionMatrix(test.outcome, hcvdat.test$outcome, positive="LiverDisease")
```

